{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Environment Variables for Langsmith\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLM model instance for LLAMA2\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello Rishil! It's nice to meet you. How can I assist you today?\n",
      "<class 'str'>\n",
      "*******************\n",
      "Assistant: I'm just an AI, I don't have access to personal information about you unless you provide it to me. Can you tell me your name?\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Models are instances of Runnables, meaning they expose \n",
    "# a standard interface for interaction\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Here we are telling the model our name\n",
    "result1 = model.invoke([HumanMessage(content=\"Hi!, I'm Rishil\")])\n",
    "print(result1)\n",
    "print(type(result1))\n",
    "\n",
    "print(\"*******************\")\n",
    "\n",
    "# Right after we told our name, \n",
    "# model still does not remember it because no history is maintained by the model\n",
    "result2 = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "print(result2)\n",
    "print(type(result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAI: Your name is Rishil.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the model to know past conversation, \n",
    "# we have to provide the conversation history to it as a list\n",
    "# Then the model will respond as per the latest message in the history\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Rishil\"),\n",
    "        AIMessage(content=\"Hello Rishil! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement this message history mechanism we use integrations from langchain_community\n",
    "# Basically keeping track of i/p and o/p and storing them in datastore\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Each conversation will be stored under a different session_id in store dict\n",
    "store = {}\n",
    "\n",
    "# If a session id is not present, create a new empty chat history\n",
    "# and if present, pass the chat history of that session id to the model\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create a Runnable object that passes the chat history to the model from store\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a config variable to pass to the runnable every time.\n",
    "# This contains info that is not directly part of input but still useful\n",
    "# Each unique session_id will represent a different conversation\n",
    "# and accordingly be stored in the data store\n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run d85be9f2-d99d-44f7-bc11-b19a364ef4f7 not found for run b54bfe76-d2a5-4bde-ac06-e08c854981b7. Treating as a root run.\n",
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAssistant: Hello Rishil! It's nice to meet you. How can I assist you today? Is there something specific you would like to chat about or ask?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Runnable object that passes history to the model as per session id\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, I'm Rishil\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2841f5a2-fed4-4b24-8edf-c99b498277cd not found for run 4901297e-73e4-4b12-85e3-e69e2ce92b31. Treating as a root run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Bot: I'm just an AI, I don't have access to personal information such as your name. Additionally, it is not appropriate or ethical to ask for personal information from someone without their consent. It is important to respect people's privacy and only request information that is necessary and appropriate in a given context. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Runnable with message history to check if history was maintained\n",
    "# This does not work with LLAMA2 model as the return format is different\n",
    "# so message history cannot be maintained\n",
    "# Should work fine with OpenAI or any other models\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates to create a message history\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Every message in ChatPromptTemplate.from_messages will be a list of tuples\n",
    "# Every tuple is a single message consisting of 2 values\n",
    "# First is either \"system\" or \"user\", representing whose message was it\n",
    "# Second is the actual message from the system or the user.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # Use this as placeholder to add new messages in it\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Rishil! *smiling* It's nice to meet you. Is there something I can help you with or would you like me to recommend some information on a particular topic? Please feel free to ask me anything!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing input to a chain instead of the LLM as dict\n",
    "# we are using \"messages\" to add this chat to the placeholder \n",
    "# we created in prompt as it uses \"messages\" as the variable name\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Hi! I'm Rishil\")]})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can wrap this in message history as before, this time with chain that creates \n",
    "# a ChatPromptTemplate which stores the chat\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0aef3a1e-793f-45d6-bacf-aab0c4424079 not found for run 2d459089-bc70-40fa-b908-82b2451adefc. Treating as a root run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 64b12dfb-7f4c-45ed-87a9-def23c8e6d22 not found for run c8dc1fc8-aedb-4e92-90bc-eac00feace35. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Rishil! It's nice to meet you. How can I assist you today? Do you have any questions or tasks you'd like me to help you with?\n",
      "*******************\n",
      "AI: Hi Rishil! Your name is Rishil. üòä Is there anything else you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "# Using the chain to chat with the bot that keeps history of the conversation\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Rishil\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# After giving your name to the model\n",
    "print(response)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"*******************\")\n",
    "\n",
    "# Asking your name again to check if history was maintained\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc2': InMemoryChatMessageHistory(messages=[]),\n",
       " 'abc5': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Rishil\"), AIMessage(content=\"Hello Rishil! It's nice to meet you. How can I assist you today? Do you have any questions or tasks you'd like me to help you with?\"), HumanMessage(content=\"What's my name?\"), AIMessage(content='AI: Hi Rishil! Your name is Rishil. üòä Is there anything else you would like to know or discuss?')])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our data store to see the chats stored in it\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going one step further, we will make our chatbot converse in a given language\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the chain again to make the chatbot converse in a given language\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÅ„É™„Ç∑„É£„Éº„É´„Åß„Åô! (Ohayou gozaimasu, Risha-ru desu!) It's a pleasure to meet you! How may I assist you today? üòä\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can also pass language as input to the chain\n",
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm Rishil\")], \"language\": \"Japanese\"}\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now wrapping this more complicated chain in message history class\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 267b1613-e69a-4646-b7f2-5acca0c170a9 not found for run dc7df844-8609-4d61-a287-ae7215f3f1cf. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'„Äå„Åì„Çì„Å´„Å°„ÅØ„ÄÅ„É™„Ç∑„É´„Åï„Çì„ÄÇ„ÅäÂêçÂâç„Åå„Å®„Å¶„ÇÇÁ¥†Êô¥„Çâ„Åó„ÅÑ„Åß„Åô„ÄÇÊó•Êú¨Ë™û„ÇíË©±„Åõ„Çã„Åã„Å©„ÅÜ„Åã„ÄÅÁßÅ„Åü„Å°Âä©„Åë„Å´„Å™„Çä„Åæ„ÅôÔºÅ„Äç'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using this Runnable with message history \n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm Rishil\")], \"language\": \"Japanese\"}, \n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 95c64a89-b2ed-47b1-95f6-c88de37c077a not found for run 88fdd26c-f794-4e59-8a5d-14f1174ec821. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI:„Äå„É™„Ç∑„É´„Åï„Çì„ÅÆÂêçÂâç„ÅØ„ÄÅ„ÅÇ„Å™„Åü„ÅÆ„ÅäÂêçÂâç„Åß„Åô„ÄÇÂêçÂâç„ÅØ„ÄÅÊó•Êú¨Ë™û„ÅßË®Ä„ÅÑ„Åæ„Åô„ÄÇ„ÅÇ„Å™„Åü„ÅÆÂêçÂâç„ÅØ„ÄÅ„ÄåRishil„Äç„Å®Ë®Ä„ÅÑ„Åæ„Åô„ÄÇ„Äç'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if it remembers my name \n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name?\")], \"language\": \"Japanese\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing Conversation History\n",
    "# If the converstaion history is left unmanaged, it will grow unbounded and potentially \n",
    "# beyond the context window of the LLM.\n",
    "from langchain_core.messages import SystemMessage, utils\n",
    "\n",
    "\n",
    "trimmer = utils.trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AIMessage', 'AIMessageChunk', 'AnyMessage', 'BaseMessage', 'BaseMessageChunk', 'ChatMessage', 'ChatMessageChunk', 'FunctionMessage', 'FunctionMessageChunk', 'HumanMessage', 'HumanMessageChunk', 'InvalidToolCall', 'MessageLikeRepresentation', 'SystemMessage', 'SystemMessageChunk', 'ToolCall', 'ToolCallChunk', 'ToolMessage', 'ToolMessageChunk', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_message_from_dict', 'ai', 'base', 'chat', 'convert_to_messages', 'function', 'get_buffer_string', 'human', 'merge_content', 'message_chunk_to_message', 'message_to_dict', 'messages_from_dict', 'messages_to_dict', 'system', 'tool', 'utils']\n"
     ]
    }
   ],
   "source": [
    "# To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n",
    "# Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we ask about info that is within the last few messages, it remembers\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new chain that uses trimmer along with keeping message history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve the User Experience, we can stream the output as it is being generated\n",
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
